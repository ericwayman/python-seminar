{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import relevant modules\n",
      "import skimage.io as io\n",
      "import numpy as np\n",
      "import re, sklearn\n",
      "import pickle\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from os import listdir\n",
      "from multiprocessing import Pool, cpu_count\n",
      "from pylab import imread\n",
      "from time import time\n",
      "\n",
      "\n",
      "#Methods to compute image features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#Build up Training Data\n",
      "\n",
      "#path to the folders of image categories\n",
      "#MYDIRECTORY = '/Users/waymaniac/Desktop/Py_for_data_sci/homework/hw4/50_categories'\n",
      "MYDIRECTORY = './50_categories'\n",
      "\n",
      "# FUNCTION DEFINITIONS\n",
      "\n",
      "\n",
      "####FEATURE FUNCTIONS\n",
      "#these functions compute and return a real numbered value feature of an image\n",
      "#all these functions take a 3d image array as an input and return a \n",
      "#float representing an image feature as an output\n",
      "\n",
      "\n",
      "####add compute max feature size relative to image size\n",
      "\n",
      "\n",
      "\n",
      "def feature_0(img):\n",
      "    \"\"\"\n",
      "    Computes the number of objects and the number of large objects using edge-based segmentation.\n",
      "    Here a large object is one of size > 20.  \n",
      "    input: img- a 2d or 3d ndarray representing a greyscale or RGB image resp.\n",
      "    Returns: a list of the form: [num_obj, big_num_obj]\n",
      "    \"\"\"\n",
      "    from scipy import ndimage\n",
      "    from skimage.filter import canny\n",
      "    # if image is color, flatten the image to give a greyscale image\n",
      "    if img.ndim ==3:\n",
      "        img = img.mean(axis=2)\n",
      "    edges = canny(img/255.)\n",
      "    fill_img = ndimage.binary_fill_holes(edges)\n",
      "    label_objects, num_obj = ndimage.measurements.label(fill_img)\n",
      "    sizes = np.bincount(label_objects.ravel())\n",
      "    mask_sizes = sizes > 20\n",
      "    mask_sizes[0] = 0\n",
      "    img_cleaned = mask_sizes[label_objects]\n",
      "    big_label_objects, big_num_obj = ndimage.measurements.label(img_cleaned)\n",
      "    max_obj = label_objects.max()\n",
      "    return [num_obj, big_num_obj]\n",
      "def feature_1(img): \n",
      "    \"\"\"Computes the average local entropy over each pixel of the image.  \n",
      "    The local entropy is computed over a disc of radius 5, and is an average of the local entropy over \n",
      "    the RGB channels or greyscale image if 2D\n",
      "    input: a color or greyscale image img. \n",
      "    returns: a list representing the avg local entropy over each of the 3 color channels\n",
      "    \"\"\"\n",
      "    import numpy as np\n",
      "    from skimage.filter.rank import entropy\n",
      "    from skimage.morphology import disk\n",
      "    if img.ndim==3:\n",
      "        ent_list = []\n",
      "        for i in [0,1,2]:\n",
      "            ent_array = entropy(img[:,:,i],disk(5))\n",
      "            ent_list.append(np.mean(ent_array))\n",
      "        return ent_list\n",
      "    else:\n",
      "        ent_array = entropy(img,disk(5))\n",
      "        return [np.mean(ent_array)]*3\n",
      "\n",
      "def feature_2(img):\n",
      "    \"\"\"Computes the maximum cross correlation between color channels i and j in the image for all color combinations\n",
      "    inputs: a 3 color channel image: img, the indices of the channels for which the cross cor is computed: i,j\n",
      "    returns: a list of the maximum correlations of the normalized images corresponding to channels i and j\n",
      "    \"\"\"\n",
      "    import numpy as np\n",
      "    from scipy.signal import fftconvolve as fftc\n",
      "    #If only one color channel just return 0\n",
      "    if img.ndim==2:\n",
      "        return [0]*6\n",
      "    else:\n",
      "        feature_list = []\n",
      "        for x in [(0,0),(0,1),(0,2),(1,1),(1,2),(2,2)]:\n",
      "            img_1 = img[:,:,x[0]]\n",
      "            img_2 = img[:,:,x[1]]\n",
      "            #normalize the two color channels\n",
      "            im1_norm = (img_1-img_1.mean())/img_1.std()\n",
      "            im2_norm = (img_2-img_2.mean())/img_2.std()\n",
      "            corr = fftc(im1_norm,im2_norm, mode = 'same') #cross cor of the base image\n",
      "            feature_list.append(np.max(corr))\n",
      "        return feature_list\n",
      "\n",
      "def feature_3(img):\n",
      "    feature = img.size\n",
      "    return [feature]\n",
      "\n",
      "def compute_features(img, num_funct=4):\n",
      "    \"\"\"\n",
      "    inputs: \n",
      "        num_funct: the number of feature producing functions (some functs return a list of more than one feature\n",
      "        image: a 3 dim RGB image array on which to compute a list of features.\n",
      "    Returns: a list of length num_feat containing the computed features for the image \n",
      "    Assumes the features are given by functions labeled \"feature_i\" for i=0,..,num_funct-1\n",
      "    and that these feature functions return a list (possibly of only one element) containing float valued features.\n",
      "    \"\"\"\n",
      "    feature_list =[]\n",
      "    for i in range(num_funct):\n",
      "        func_name = 'feature_%s' %str(i)\n",
      "        feature = globals()[func_name](img)\n",
      "        for j in range(len(feature)):\n",
      "            feature_list.append(feature[j])   \n",
      "    return feature_list\n",
      "\n",
      "\n",
      "# Quick function to divide up a large list into multiple small lists, \n",
      "# attempting to keep them all the same size. \n",
      "def split_seq(seq, size):\n",
      "        newseq = []\n",
      "        splitsize = 1.0/size*len(seq)\n",
      "        for i in range(size):\n",
      "            newseq.append(seq[int(round(i*splitsize)):\n",
      "                int(round((i+1)*splitsize))])\n",
      "        return newseq\n",
      "# Our simple feature extraction function. It takes in a list of image paths, \n",
      "# does some measurement on each image, then returns a list of the image paths\n",
      "# paired with the results of the feature measurement.\n",
      "\n",
      "def extract_training_features(image_path_list):\n",
      "    \"\"\"\n",
      "    inputs: image_path_list: a directory containing images files to be extracted\n",
      "    Returns: feature_list a list of length equal to the number of images in image_path_list\n",
      "        Each element of feature list is a list containing the path to the image and the features \n",
      "        for the corresponding image\n",
      "        Keeps the path and features grouped together so we can shuffle the list and keep the\n",
      "        image path and correpsonding features together\n",
      "    \"\"\"\n",
      "    path_feat_list = []\n",
      "    for image_path in image_path_list:\n",
      "        image_array = imread(image_path)\n",
      "        feature_list = compute_features(image_array)\n",
      "        feature_list.insert(0,image_path)\n",
      "        path_feat_list.append(feature_list)\n",
      "    return path_feat_list\n",
      "\n",
      "def extract_test_features(image_path_list):\n",
      "    \"\"\"\n",
      "    This function is similar to extract_training_features, except we only extract a list of features\n",
      "    we don't include the image paths because we don't need the labels\n",
      "    inputs: image_path_list: a directory containing images files to be extracted\n",
      "    Returns: feature_list a list of length equal to the number of images in image_path_list\n",
      "        Each element of feature list is a list containing the path to the image and the features \n",
      "        for the corresponding image\n",
      "        Keeps the path and features grouped together so we can shuffle the list and keep the\n",
      "        image path and correpsonding features together\n",
      "    \"\"\"\n",
      "    path_feat_list = []\n",
      "    for image_path in image_path_list:\n",
      "        image_array = imread(image_path)\n",
      "        feature_list = compute_features(image_array)\n",
      "        path_feat_list.append(feature_list)\n",
      "    return path_feat_list\n",
      "\n",
      "\n",
      "\n",
      "### Main program starts here ###################################################\n",
      "###This program trains a random tree classifier model\n",
      "### on the images in the folder 50_categories and estimates the accuracy \n",
      "###using cross validation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# We first collect all the local paths to all the images in one list\n",
      "image_paths = []\n",
      "keys = []\n",
      "categories = listdir(MYDIRECTORY)\n",
      "for category in categories:\n",
      "    #make sure we don't grab the .DS_Store file which isn't a directory\n",
      "    if category != '.DS_Store':\n",
      "    \tkeys.append(category)\n",
      "        image_names = listdir(MYDIRECTORY  + \"/\" + category)\n",
      "    \tfor name in image_names:\n",
      "        \timage_paths.append(MYDIRECTORY + \"/\" + category + \"/\" + name)\n",
      "print (\"There should be 4244 images, actual number is \" + \n",
      "    str(len(image_paths)) + \".\")\n",
      "values = range(len(keys))\n",
      "label_dict = dict(zip(keys,values))\n",
      "print (\"the dictionary for the classification labels: \" + str(label_dict))\n",
      "\n",
      "# Then, we run the feature extraction function using multiprocessing.Pool so \n",
      "# so that we can parallelize the process and run it much faster.\n",
      "numprocessors = cpu_count() # To see results of parallelizing, set numprocessors\n",
      "                            # to less than cpu_count().\n",
      "# numprocessors = 1\n",
      "\n",
      "# We have to cut up the image_paths list into the number of processes we want to\n",
      "# run. \n",
      "split_image_paths = split_seq(image_paths, numprocessors)\n",
      "\n",
      "# Ok, this block is where the parallel code runs. We time it so we can get a \n",
      "# feel for the speed up.\n",
      "start_time = time()\n",
      "p = Pool(numprocessors)\n",
      "result = p.map_async(extract_training_features, split_image_paths)\n",
      "poolresult = result.get()\n",
      "end_time = time()\n",
      "\n",
      "# All done, print timing results.\n",
      "print (\"Finished extracting features. Total time: \" + \n",
      "    str(round(end_time-start_time, 3)) + \" s, or \" + \n",
      "    str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
      "# This took about 10-11 seconds on my 2.2 GHz, Core i7 MacBook Pro. It may also\n",
      "# be affected by hard disk read speeds.\n",
      "\n",
      "# To tidy-up a bit, we loop through the poolresult to create a final list of\n",
      "# the feature extraction results for all images.\n",
      "combined_result = []\n",
      "for single_proc_result in poolresult:\n",
      "    for single_image_result in single_proc_result:\n",
      "        combined_result.append(single_image_result)\n",
      "\n",
      "\n",
      "#Split the results into an array of the sample labels: label_list\n",
      "# and an array of the sample features: X\n",
      "np.random.shuffle(combined_result)\n",
      "result_array = np.array(combined_result)\n",
      "print \"The first 5 samples from the result are are:  \"+ str(result_array[0:5,])\n",
      "Y = result_array[:,0]\n",
      "X = result_array[:,1:]\n",
      "regex = re.compile('50_categories/(.*?)/')\n",
      "#find the list of labels in the 4244 training images\n",
      "targ_lab_keys = [regex.search(Y[i]).group(1) for i in range(len(Y))]\n",
      "#return the integer keys associated with each label in the training images\n",
      "targ_lab_vals = [label_dict.get(val) for val in targ_lab_keys] \n",
      "#convert the list of label keys to an array to be used for the scikit functions\n",
      "targ_array = np.asarray(targ_lab_vals)\n",
      "#print \"the first labels of the first five samples are: \"+ str(targ_lab_keys[0:5])\n",
      "#print \"the first label values of the first five samples are: \"+str(targ_lab_vals[0:5])\n",
      "#print \"The features of the first five samples are: \"+ str(X[0:5,:])\n",
      "\n",
      "#Make the Random tree classifier \n",
      "clf = RandomForestClassifier(n_estimators=50,n_jobs=-1,compute_importances=True)\n",
      "clf = clf.fit(X,targ_array)\n",
      "#print \"The 50 Label classes are:\"+str(clf.classes_)\n",
      "scores = sklearn.cross_validation.cross_val_score(clf,X,targ_array)\n",
      "print(\"Accuracy: %.2f (+/-%.2f)\" %(scores.mean(),scores.std()*2))\n",
      "print scores\n",
      "pickle.dump(clf, open('trained_classifier.p','w'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_final_classifier(path,forest):\n",
      "    \"\"\"Using the Random forest classifier model trained on the images in the folder\n",
      "    '50_categories', this function returns and prints the predictions for the labels\n",
      "    from the images located in 'path'.\n",
      "    inputs: path: the path where the test images are located.  \n",
      "                Assumes all test images are located in a single directory in path\n",
      "                that is, none of the images should be located in subdirectories within path\n",
      "            forest: the RandomForestClassifier object trained on the image set in \n",
      "                    '50_categories'\n",
      "    Returns: The predicted labels for the images in path.                                    \n",
      "    \"\"\"\n",
      "    label_dict =   {\n",
      "    0: 'airplanes', 1: 'bat', 2: 'bear', 3: 'blimp', 4: 'camel', 5: 'comet', 6: 'conch', 7: 'cormorant', 8: \n",
      "    'crab', 9: 'dog', 10: 'dolphin', 11: 'duck', 12: 'elephant', 13: 'elk', 14: 'frog', 15: 'galaxy', 16: \n",
      "    'giraffe', 17: 'goat', 18: 'goldfish', 19: 'goose', 20: 'gorilla', 21: 'helicopter', 22: 'horse', 23: \n",
      "    'hot-air-balloon', 24: 'hummingbird', 25: 'iguana', 26: 'kangaroo', 27: 'killer-whale', 28: 'leopards', \n",
      "    29: 'llama', 30: 'mars', 31: 'mussels', 32: 'octopus', 33: 'ostrich', 34: 'owl', 35: 'penguin', 36: \n",
      "    'porcupine', 37: 'raccoon', 38: 'saturn', 39: 'skunk', 40: 'snail', 41: 'snake', 42: 'speed-boat', \n",
      "    43: 'starfish', 44: 'swan', 45: 'teddy-bear', 46: 'toad', 47: 'triceratops', 48: 'unicorn', 49: 'zebra'\n",
      "    }\n",
      "\n",
      "\n",
      "    image_paths = []\n",
      "    file_name_list = []\n",
      "    image_names = listdir(path)\n",
      "    for name in image_names:\n",
      "        image_paths.append(path + \"/\" + name)\n",
      "        file_name_list.append(name)\n",
      "    print (\"collected pathes for %s test images\" %str(len(image_paths)))\n",
      "    \n",
      "\n",
      "    # Then, we run the feature extraction function using multiprocessing.Pool so \n",
      "    # so that we can parallelize the process and run it much faster.\n",
      "    numprocessors = cpu_count() # To see results of parallelizing, set numprocessors\n",
      "                                # to less than cpu_count().\n",
      "    # numprocessors = 1\n",
      "\n",
      "    # We have to cut up the image_paths list into the number of processes we want to\n",
      "    # run. \n",
      "    split_image_paths = split_seq(image_paths, numprocessors)\n",
      "\n",
      "    # Ok, this block is where the parallel code runs. We time it so we can get a \n",
      "    # feel for the speed up.\n",
      "    start_time = time()\n",
      "    p = Pool(numprocessors)\n",
      "    result = p.map_async(extract_test_features, split_image_paths)\n",
      "    poolresult = result.get()\n",
      "    end_time = time()\n",
      "\n",
      "    # All done, print timing results.\n",
      "    print (\"Finished extracting features. Total time: \" + \n",
      "        str(round(end_time-start_time, 3)) + \" s, or \" + \n",
      "        str( round( (end_time-start_time)/len(image_paths), 5 ) ) + \" s/image.\")\n",
      "\n",
      "\n",
      "    # To tidy-up a bit, we loop through the poolresult to create a final list of\n",
      "    # the feature extraction results for all images.\n",
      "    combined_result = []\n",
      "    for single_proc_result in poolresult:\n",
      "        for single_image_result in single_proc_result:\n",
      "            combined_result.append(single_image_result)\n",
      "    test_feat_array = np.array(combined_result)\n",
      "    prediction = forest.predict(test_feat_array)\n",
      "    #print(\"the first 5 predicted numeric labels are:\")\n",
      "    #print prediction[0:5]\n",
      "    print(\"filename       predicted_class\")\n",
      "    print(\"________________________________\")\n",
      "    for i in range(len(file_name_list)):\n",
      "        print(\"%s          %s\" %(file_name_list[i],label_dict[prediction[i]]))\n",
      "#path = './50_categories/goose'\n",
      "#run_final_classifier(path,clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}